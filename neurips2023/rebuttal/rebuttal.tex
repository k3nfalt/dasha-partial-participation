\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{footnote}
\usepackage{mathtools}
\usepackage[flushleft]{threeparttable}
\usepackage{etoolbox}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage[margin=0.5in]{geometry}

\include{macros}

\begin{document}
\begin{center}
    Extra Experiments
\end{center}

In this experiments, we compare our new algorithm \algname{DASHA-PP} with previous baselines \algname{MARINA} and \algname{FRECON} in the partial participation setting (the comparisons of \algname{DASHA-PP} with \algname{DASHA} we present in Section~A of the paper). We consider \algname{MARINA} and \algname{FRECON} because they are the previous SOTA methods in the \emph{partial participation setting with {\bf compression}}. We investigate the same optimization problem and setup as in Section~A of the paper. All methods use the Rand$K$ compressor in these experiments.
% \section*{Warm Up: Gradient Setting}
% \begin{figure}[h]
% \includegraphics[width=\textwidth]{neurips_2022_gradient_mushrooms_nodes_100_coord_10_prob_0_01_seeds_3_no_init_with_pp_marina.pdf}
% \caption{Classification task with the \textit{mushrooms} dataset.}
% \label{fig:gradient}
% \end{figure}

\begin{center}
    \bf Stochastic Setting
\end{center}

In Figure~\ref{fig:stochastic}, we compare all three methods in the stochastic setting on two different datasets: \textit{real-sim} and \textit{MNIST}. The parameter $s$ means the number of clients participating in each round. We can see that \algname{DASHA-PP} has better convergence rates than \algname{FRECON}. \algname{DASHA-PP} with $s \in \{5, 10\}$ even improves \algname{MARINA} that works in the full participation regime (one should expect that convergence rate of \algname{MARINA} with partial participation will be even worse).

\begin{figure}[H]
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\textwidth]{tmp_launch/neurips_2022_stochastic_real-sim_nof_200_numnodes_10_probs_mega_batch_100000_fix_nm_bug.pdf}
        \caption{\textit{real-sim}}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\textwidth]{tmp_launch/neurips_2023_stochastic_mnist_nof_100_numnodes_10_probs_mega_batch_100000_batch_size_10_longer.pdf}
        \caption{\textit{MNIST}}
    \end{subfigure}
\caption{Classification task}
\label{fig:stochastic}
\end{figure}

\begin{center}
    \bf Finite-Sum Setting
\end{center}
In Figure~\ref{fig:finite-sum}, we consider the finite-sum setting. 
We can see that \algname{DASHA-PP} with the number of participated clients $s \in \{50, 90, 100\}$ (in \textit{real-sim}) and $s \in \{5, 9, 10\}$ (in \textit{MNIST}) converges faster than \algname{MARINA} (that works in the full participation regime!). Since \algname{FRECON} does not support variance reduction of stochastic gradients, it converges to less accurate solutions..
% We do not have experiments with \algname{FRECON} because the authors of \algname{FRECON} do not consider it in the finite-sum setting. We can see that \algname{DASHA-PP} with $s \in \{50, 90, 100\}$ converges faster than \algname{MARINA} (that works in the full participation regime).

\begin{figure}[H]
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\textwidth]{neurips_2022_finite_sum_real-sim_nof_500_numnodes_100_more_probs_batch_size_1_frecon.pdf}
        \caption{\textit{real-sim}}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\textwidth]{neurips_2022_finite_sum_mnist_nof_1000_numnodes_10_more_probs_batch_size_100_split_by_labels_logistic_frecon.pdf}
        \caption{\textit{MNIST}}
    \end{subfigure}
\caption{Classification task}
\label{fig:finite-sum}
\end{figure}

% \begin{figure}[H]
%     \begin{subfigure}{.5\textwidth}
%         \includegraphics[width=\textwidth]{tmp_launch/neurips_2022_finite_sum_real-sim_nof_500_numnodes_100_more_probs_batch_size_1.pdf}
%         \caption{\textit{real-sim}}
%     \end{subfigure}
%     \begin{subfigure}{.5\textwidth}
%         \includegraphics[width=\textwidth]{tmp_launch/neurips_2022_finite_sum_mnist_nof_7850_numnodes_10_more_probs_batch_size_100_split_by_labels_logistic.pdf}
%         \caption{\textit{MNIST}}
%     \end{subfigure}
% \caption{Classification task}
% \label{fig:finite-sum}
% \end{figure}

% \begin{figure}[H]
%     \begin{subfigure}{.5\textwidth}
%         \includegraphics[width=\textwidth]{neurips_2022_finite_sum_real-sim_nof_500_numnodes_100_more_probs_batch_size_1.pdf}
%         \caption{REAL-SIM, BZ = 1}
%     \end{subfigure}
%     \begin{subfigure}{.5\textwidth}
%         \includegraphics[width=\textwidth]{neurips_2022_finite_sum_real-sim_nof_500_numnodes_100_more_probs_batch_size_1000.pdf}
%         \caption{REAL-SIM, BZ = 1000}
%     \end{subfigure}
% \caption{Classification task with the \textit{mushrooms} dataset.}
% \label{fig:finite-sum-real-sim}
% \end{figure}

\end{document}
