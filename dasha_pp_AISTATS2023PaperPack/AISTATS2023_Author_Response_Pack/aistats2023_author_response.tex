\documentclass{article}

\usepackage{aistats2023_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % define colors in text
\usepackage{xspace}         % fix spacing around commands

\newcommand{\cC}{\mathcal{C}}

\begin{document}

% You have until \textbf{Monday, December 5, 2022 (11:59PM Anywhere on Earth)} to (optionally) respond to the reviews. You must submit a single response that addresses all reviews (not one response per review). The author response is limited to a \textbf{single page} in PDF format, including all figures, tables, and references, and has to use the AISTATS ``author response'' style that accompanies this \texttt{tex}-file. You may not alter this style file; in particular, you may not change the paper size, font, font size, or margins. Moreover, author responses must not contain external links, and must be \textbf{anonymized}.

% Please focus your response on either answering specific questions raised in the reviews or correcting any misunderstanding or factual errors in the reviews.

% You can change your response as often as you like until the above deadline. Please note that \textbf{this deadline is strict} and we encourage you to submit your response early so as to avoid technical issues. Please be aware that the deadline is \textbf{11:59PM Anywhere on Earth}.


% To include a figure in your response, the following LaTeX code is a possible solution:

% \begin{verbatim}
% \begin{minipage}[b]{0.3\linewidth}
% \includegraphics[width=\linewidth]{path_to_figure}
% \captionof{figure}{figure_caption}
% \end{minipage}
% \end{verbatim}

We want to thank the reviewers for their comments.

\textbf{Reviewer #2:}

1. (Experiments) (Challenge in Theoretical Analysis)

2.a. ``There are eight assumptions used in the analysis which might be too restrictive.''

In Tables 1 and 2, in column ``Limitations'', we compare how the assumptions between the papers are different. The main purpose of ``Limitations'' is to show that our assumptions are not stronger than the assumptions of any papers from Tables 1 and 2, so we do not agree that they are restrictive. Some papers present these eight assumptions as two-three assumptions. For instance, we define 3 different smoothness constants, while other papers can define only 1 constant, which is just the maximum of our 3 smoothness constants.

2.b. ``In particular, could you please provide more comments on Assumption 6?.''
This is the mean-squared smoothness property that is used in \textbf{all papers with variance reduction}. See (Lower Bounds for Non-Convex Stochastic Optimization Arjevani et al.; Momentum-Based Variance Reduction in Non-Convex SGD
Cutkosky et al., SPIDER: Near-optimal non-convex optimization via stochastic path integrated differential estimator 
Fang et al.)

3. This part is indeed can be confusing. We mean that the compressors are \emph{statistically} independent. In other words, $\cC_1(x_1), \dots, \cC_n(x_n)$ are independent random vectors.

4. In order to get the computational and communication complexities, we have to substitute an explicit formula of $\omega$ from Definition 1. It may differ for different compressors (see On Biased Compression for Distributed Learning Beznosikov et al). So it is not possible to provide a nice corollary that will work for any compressor. The good news is that RandK is the simplest compressor. By showing an improvement for RandK, we can expect that more advanced compressors will have even better theoretical guarantees.

5. We also discuss it after Corollaries 1, 3, and 4.

\textbf{Reviewer #4:}

1. (Challenge in Theoretical Analysis)

2. If you read papers from Tables 1 and 2, you will find that virtually all of them extend the theory from some previous papers. The research on the optimization methods is incremental. It is not very obvious why it is a weakness of our paper and not others.

3. (Experiments)

\textbf{Reviewer #5:}

1. (Experiments)

2. We are aware of the work by A. Defazio & L. Bottou. But it does not mean that VR methods are hopeless for neural network optimization. See, for instance, a recent work (Momentum-Based Variance Reduction in Non-Convex SGD
Ashok Cutkosky, Francesco Orabona), where they provided theoretical and practical improvement. The development and understanding of VR methods are still going.

\end{document}
