@inproceedings{Transformer2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{Koloskova2019-DecentralizedEC-2019,
  author    = {Anastasia Koloskova and Sebastian U Stich and Martin Jaggi},
  booktitle = {International Conference on Machine Learning},
  title     = {Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication},
  year      = {2019},
  journal   = {International Conference on Machine Learning},
}

@inproceedings{GPT3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{SPIDER,
  author    = {Cong Fang and Chris Junchi Li and Zhouchen Lin and Tong Zhang},
  booktitle = {NeurIPS Information Processing Systems},
  title     = {{SPIDER}: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator},
  year      = {2018},
}

@InProceedings{SARAH,
  author    = {Nguyen, Lam and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  title     = {{SARAH}: A novel method for machine learning problems using stochastic recursive gradient},
  booktitle = {The 34th International Conference on Machine Learning},
  year      = {2017},
}

@Article{EF21,
  author  = {Peter Richt\'{a}rik and Igor Sokolov and Ilyas Fatkhullin},
  journal = {arXiv preprint arXiv:2106.05203},
  title   = {{EF21}: A new, simpler, theoretically better, and practically faster error feedback},
  year    = {2021},
}

@Article{FedNL,
  author  = {Safaryan, Mher and Islamov, Rustem and Qian, Xun and Richt\'{a}rik, Peter},
  journal = {arXiv preprint arXiv:2106.02969},
  title   = {{FedNL}: Making {N}ewton-type methods applicable to federated learning},
  year    = {2021},
}

@InProceedings{ADIANA,
  author    = {Zhize Li and Dmitry Kovalev and Xun Qian and Peter Richt\'{a}rik},
  booktitle = {International Conference on Machine Learning},
  title     = {Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization},
  year      = {2020},
}

@Article{DCGD,
  author  = {Sarit Khirirat and Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal = {arXiv preprint arXiv:1806.06573},
  title   = {Distributed learning with compressed gradients},
  year    = {2018},
}

@InProceedings{Alistarh-EF-NIPS2018,
  author    = {Alistarh, Dan and Hoefler, Torsten and Johansson,  Mikael and Khirirat,  Sarit and Konstantinov,  Nikola and Renggli,  C\'{e}dric},
    title     = {The convergence of sparsified gradient methods},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2018},
}

@inproceedings{Seide2014,
  author    = {Seide,  Frank and Fu,  Hao and Droppo,  Jasha and Li,  Gang and Yu,  Dong},
  title     = {1-bit stochastic gradient descent and its application to data-parallel distributed training of speech {DNN}s},
  booktitle = {Fifteenth Annual Conference of the International Speech Communication Association},
  year      = {2014},
}

@InProceedings{PowerSGD,
  author    = {Thijs Vogels and Sai Praneeth Karimireddy and Martin Jaggi},
  booktitle = {Neural Information Processing Systems},
  title     = {Power{SGD}: Practical Low-Rank Gradient Compression for Distributed Optimization},
  year      = {2019},
}

@InProceedings{MARINA,
  author    = {Eduard Gorbunov and Konstantin Burlachenko and Zhize Li and Peter Richt\'{a}rik},
  booktitle = {38th International Conference on Machine Learning},
  title     = {{MARINA}: {F}aster non-convex distributed learning with compression},
  year      = {2021},
}

@InProceedings{EC-SGD,
  author    = {Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt\'{a}rik, Peter},
  booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
  title     = {Linearly Converging Error Compensated {SGD}},
  year      = {2020},
}

@Article{EC-LSVRG,
  author  = {Xun Qian and Hanze Dong and Peter Richt\'{a}rik and Tong Zhang},
  journal = {OPT2020: 12th Annual Workshop on Optimization for Machine Learning (NeurIPS 2020 Workshop)},
  title   = {Error compensated loopless {SVRG} for distributed optimization},
  year    = {2020},
}

@InProceedings{DoubleSqueeze2019,
  author    = {Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {DoubleSqueeze: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression},
  year      = {2019},
  address   = {Long Beach, California, USA},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month     = {09--15 Jun},
  pages     = {6155--6165},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {A standard approach in large scale machine learning is distributed stochastic gradient training, which requires the computation of aggregated stochastic gradients over multiple nodes on a network. Communication is a major bottleneck in such applications, and in recent years, compressed stochastic gradient methods such as QSGD (quantized SGD) and sparse SGD have been proposed to reduce communication. It was also shown that error compensation can be combined with compression to achieve better convergence in a scheme that each node compresses its local stochastic gradient and broadcast the result to all other nodes over the network in a single pass. However, such a single pass broadcast approach is not realistic in many practical implementations. For example, under the popular parameter-server model for distributed learning, the worker nodes need to send the compressed local gradients to the parameter server, which performs the aggregation. The parameter server has to compress the aggregated stochastic gradient again before sending it back to the worker nodes. In this work, we provide a detailed analysis on this two-pass communication model, with error-compensated compression both on the worker nodes and on the parameter server. We show that the error-compensated stochastic gradient algorithm admits three very nice properties: 1) it is compatible with an \emph{arbitrary} compression technique; 2) it admits an improved convergence rate than the non error-compensated stochastic gradient method such as QSGD and sparse SGD; 3) it admits linear speedup with respect to the number of workers. The empirical study is also conducted to validate our theoretical results.},
  file      = {tang19d.pdf:http\://proceedings.mlr.press/v97/tang19d/tang19d.pdf:PDF},
  url       = {http://proceedings.mlr.press/v97/tang19d.html},
}

@InProceedings{errorSGD,
  author    = {Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong},
  title     = {Error Compensated Quantized {SGD} and its Applications to Large-scale Distributed Optimization},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  pages     = {5325--5333},
  address   = {Stockholmsm√§ssan, Stockholm Sweden},
  month     = {10--15 Jul},
  publisher = {PMLR},
}

@Article{Artemis2020,
  author  = {Constantin Philippenko and Aymeric Dieuleveut},
  journal = {arXiv preprint arXiv:2006.14591},
  title   = {Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees},
  year    = {2020},
}

@Article{Cnat,
  author  = {Samuel Horv\'{a}th and Chen-Yu Ho and \v{L}udov\'{i}t Horv\'{a}th and Atal Narayan Sahu and Marco Canini and Peter Richt\'{a}rik},
  title   = {Natural compression for distributed deep learning},
  journal = {arXiv preprint arXiv:1905.10988},
  year    = {2019},
}

@Article{DIANA,
  author  = {Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal = {arXiv preprint arXiv:1901.09269},
  title   = {Distributed Learning with Compressed Gradient Differences},
  year    = {2019},
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{khaled2020better,
	title={Better Theory for {SGD} in the Nonconvex World},
	author={Khaled,  Ahmed and Richt{\'a}rik,  Peter},
	journal={arXiv preprint arXiv:2002.03329},
	year={2020}
}

@article{stich2020biased,
	title={Analysis of {SGD} with Biased Gradient Estimators},
	author={Ajalloeian,  Ahmad and Stich,  Sebastian U},
	journal={arXiv preprint arXiv:2008.00051},
	year={2020}
}

% Provides with the rate of nonconvex SGD unbiased compressor

@article{beznosikov2020biased,
	title={On Biased Compression for Distributed Learning},
	author={Beznosikov,  Aleksandr and Horv{\'a}th,  Samuel and Richt{\'a}rik,  Peter and Safaryan,  Mher},
	journal={arXiv preprint arXiv:2002.12410},
	year={2020}
}

@Article{UP2021,
  author  = {Mher Safaryan and Egor Shulgin and Peter Richt\'{a}rik},
  journal = {Information and Inference: A Journal of the IMA},
  title   = {Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor},
  year    = {2021},
}

@inproceedings{PAGE,
  title={PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization},
  author={Li, Zhize and Bao, Hongyan and Zhang, Xiangliang and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={6286--6295},
  year={2021},
  organization={PMLR}
}

%We use Lemma 2 (from Appendix A) in our work.


@article{Stich2019TheEF,
  title={The Error-Feedback Framework: Better Rates for {SGD} with Delayed Gradients and Compressed Communication},
  author={Stich,  Sebastian and Sai Praneeth Karimireddy},
  journal={arXiv preprint arXiv:1909.05350},
  year={2019},
}

@InProceedings{Karimireddy_SignSGD,
  title = 	 {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes},
  author =       {Karimireddy,  Sai Praneeth and Rebjock,  Quentin and Stich,  Sebastian and Jaggi,  Martin},
 booktitle = 	 {36th International Conference on Machine Learning (ICML)},
   year = 	 {2019},
}

@inproceedings{A_better_alternative,
  title={A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning},
  author={Horv\'{a}th,  Samuel and Richt\'{a}rik,  Peter},
  booktitle={9th International Conference on Learning Representations (ICLR)},
  year={2021},
}

@inproceedings{DoubleSqueeze,
  title={{D}ouble{S}queeze: {P}arallel Stochastic Gradient Descent with Double-Pass Error-Compensated Compression},
  author={Hanlin Tang and Xiangru Lian and Chen Yu and Tong Zhang and Ji Liu},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year={2020},
}


@InProceedings{Stich-EF-NIPS2018,
  author    = {Stich,  Sebastian U.  and Cordonnier,  J.-B.  and Jaggi,  Martin},
  title     = {Sparsified {SGD} with memory},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2018},
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}


@InProceedings{Lin_EC_SGD,
  author    = {Gorbunov,  Eduard and Kovalev,  Dmitry and Makarenko,  Dmitry and Richt\'{a}rik,  Peter},
  booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS)},
  title     = {Linearly Converging Error Compensated {SGD}},
  year      = {2020},
}

@inproceedings{Koloskova2019DecentralizedDL,
  title={Decentralized Deep Learning with Arbitrary Communication Compression},
  author={Anastasia Koloskova and Tao Lin and S. Stich and Martin Jaggi},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
}

@InProceedings{alistarh2017qsgd,
  author    = {Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  title     = {{QSGD}: {C}ommunication-efficient {SGD} via gradient quantization and encoding},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2017},
  pages     = {1709--1720},
}

@InProceedings{Qsparse_local_SGD,
  title={{Q}sparse-local-{SGD}: {D}istributed {SGD} with Quantization,  Sparsification,  and Local Computations},
  author={Debraj Basu and Deepesh Data and Can Karakus and Suhas Diggavi},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019},
}


@InProceedings{CSER,
  title={{CSER:} {C}ommunication-efficient {SGD} with Error Reset},
  author={Cong Xie and Shuai Zheng and Oluwasanmi Koyejo and Indranil Gupta and Mu Li and Haibin Lin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages = {12593--12603},
  year={2020},
}


@InProceedings{induced,
  author    = {Horv\'{a}th, Samuel and Richt\'{a}rik, Peter},
  booktitle = {9th International Conference on Learning Representations (ICLR)},
  title     = {A better alternative to error feedback for communication-efficient distributed learning},
  year      = {2021},
}


@article{chang2011libsvm,
	title={{LIBSVM}: a library for support vector machines},
	author={Chang, Chih-Chung and Lin, Chih-Jen},
	journal={{ACM} {T}ransactions on {I}ntelligent {S}ystems and {T}echnology (TIST)},
	volume={2},
	number={3},
	pages={1--27},
	year={2011},
	publisher={ACM New York, NY, USA}
}
@article{tran2019hybrid,
	title={Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization},
	author={Tran-Dinh, Quoc and Pham, Nhan H and Phan, Dzung T and Nguyen, Lam M},
	journal={arXiv preprint arXiv:1905.05920},
	year={2019}
}

@book{nesterov2018lectures,
	title={Lectures on convex optimization},
	author={Nesterov, Yurii and others},
	volume={137},
	year={2018},
	publisher={Springer}
}

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages={770--778},
	year={2016}
}

@techreport{krizhevsky2009learning,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	jnumber = {Technical Report TR-2009},
	institution = {University of Toronto,  Toronto}
}

@inproceedings{paszke2019pytorch,
	title={Pytorch: An imperative style, high-performance deep learning library},
	author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
	year={2019}
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}

@book{knuth1997Art,
  author = {Knuth, Donald E.},
  title = {The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms},
  year = {1997},
  isbn = {0201896842},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address = {USA}
}

@article{fisher1938statistical,
  title={Statistical tables for biological, agricultural aad medical research},
  author={Fisher, Ronald A and Yates, Frank},
  year={1938},
  publisher={Oliver and Boyd}
}

@article{khanduri2020distributed,
  title={Distributed stochastic non-convex optimization: Momentum-based variance reduction},
  author={Khanduri, Prashant and Sharma, Pranay and Kafle, Swatantra and Bulusu, Saikiran and Rajawat, Ketan and Varshney, Pramod K},
  journal={arXiv preprint arXiv:2005.00224},
  year={2020}
}

@article{sharma2019parallel,
  title={Parallel Restarted SPIDER--Communication Efficient Distributed Nonconvex Optimization with Optimal Computation Complexity},
  author={Sharma, Pranay and Kafle, Swatantra and Khanduri, Prashant and Bulusu, Saikiran and Rajawat, Ketan and Varshney, Pramod K},
  journal={arXiv preprint arXiv:1912.06036},
  year={2019}
}

@article{li2021zerosarah,
  title={ZeroSARAH: Efficient nonconvex finite-sum optimization with zero full gradient computation},
  author={Li, Zhize and Hanzely, Slavom{\'\i}r and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2103.01447},
  year={2021}
}

@article{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex {SGD}},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={arXiv preprint arXiv:1905.10018},
  year={2019}
}

@article{arjevani2019lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={arXiv preprint arXiv:1912.02365},
  year={2019}
}

@techreport{xu2020compressed,
  title={Compressed communication for distributed deep learning: Survey and quantitative evaluation},
  author={Xu, Hang and Ho, Chen-Yu and Abdelmoniem, Ahmed M and Dutta, Aritra and Bergou, El Houcine and Karatsenidis, Konstantinos and Canini, Marco and Kalnis, Panos},
  year={2020}
}

@article{richtarik2021ef21,
  title={EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  journal={In Neural Information Processing Systems, 2021.},
  year={2021}
}

@article{horvath2019stochastic,
  title={Stochastic distributed learning with gradient quantization and variance reduction},
  author={Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1904.05115},
  year={2019}
}

@inproceedings{haddadpour2021federated,
  title={Federated learning with compression: Unified analysis and sharp guarantees},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2350--2358},
  year={2021},
  organization={PMLR}
}

@article{das2020improved,
  title={Improved convergence rates for non-convex federated learning with compression},
  author={Das, Rudrajit and Hashemi, Abolfazl and Sanghavi, Sujay and Dhillon, Inderjit S},
  journal={arXiv e-prints},
  pages={arXiv--2012},
  year={2020}
}

@article{gorbunov2021marina,
  title={MARINA: Faster non-convex distributed learning with compression},
  author={Gorbunov, Eduard and Burlachenko, Konstantin and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2102.07845},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{bubeck2021universal,
  title={A Universal Law of Robustness via Isoperimetry},
  author={Bubeck, S{\'e}bastien and Sellke, Mark},
  journal={arXiv preprint arXiv:2105.12806},
  year={2021}
}

@article{danilova2020recent,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  journal={arXiv preprint arXiv:2012.06188},
  year={2020}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\'y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{wang2021field,
  title={A field guide to federated optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}

@article{bonawitz2019towards,
  title={Towards federated learning at scale: System design},
  author={Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Kone{\v{c}}n{\'y}, Jakub and Mazzocchi, Stefano and McMahan, H Brendan and others},
  journal={arXiv preprint arXiv:1902.01046},
  year={2019}
}

@article{li2020federated,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}

@article{vogels2021relaysum,
  title={Relaysum for decentralized deep learning on heterogeneous data},
  author={Vogels, Thijs and He, Lie and Koloskova, Anastasiia and Karimireddy, Sai Praneeth and Lin, Tao and Stich, Sebastian U and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{nedic2014distributed,
  title={Distributed optimization over time-varying directed graphs},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex},
  journal={IEEE Transactions on Automatic Control},
  volume={60},
  number={3},
  pages={601--615},
  year={2014},
  publisher={IEEE}
}

@article{tran2021hybrid,
  title={A hybrid stochastic optimization framework for composite nonconvex optimization},
  author={Tran-Dinh, Quoc and Pham, Nhan H and Phan, Dzung T and Nguyen, Lam M},
  journal={Mathematical Programming},
  pages={1--67},
  year={2021},
  publisher={Springer}
}

@article{liu2020optimal,
  title={An optimal hybrid variance-reduced algorithm for stochastic composite nonconvex optimization},
  author={Liu, Deyi and Nguyen, Lam M and Tran-Dinh, Quoc},
  journal={arXiv preprint arXiv:2008.09055},
  year={2020}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@article{tyurin2022dasha,
  title={DASHA: Distributed Nonconvex Optimization with Communication Compression, Optimal Oracle Complexity, and No Client Synchronization},
  author={Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2202.01268},
  year={2022}
}

@article{wangni2018gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{horvath2019natural,
  title={Natural compression for distributed deep learning},
  author={Horv{\'a}th, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1905.10988},
  year={2019}
}

@article{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{szlendak2021permutation,
  title={Permutation Compressors for Provably Faster Distributed Nonconvex Optimization},
  author={Szlendak, Rafa{\l} and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2110.03300},
  year={2021}
}

@article{suresh2022correlated,
  title={Correlated quantization for distributed mean estimation and optimization},
  author={Suresh, Ananda Theertha and Sun, Ziteng and Ro, Jae Hun and Yu, Felix},
  journal={arXiv preprint arXiv:2203.04925},
  year={2022}
}

@article{zhao2021faster,
  title={Faster Rates for Compressed Federated Learning with Client-Variance Reduction},
  author={Zhao, Haoyu and Burlachenko, Konstantin and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2112.13097},
  year={2021}
}

@article{fatkhullin2021ef21,
  title={EF21 with bells \& whistles: Practical algorithmic extensions of modern error feedback},
  author={Fatkhullin, Ilyas and Sokolov, Igor and Gorbunov, Eduard and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2110.03294},
  year={2021}
}

@article{reddi2020adaptive,
  title={Adaptive federated optimization},
  author={Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v{c}}n{\`y}, Jakub and Kumar, Sanjiv and McMahan, H Brendan},
  journal={arXiv preprint arXiv:2003.00295},
  year={2020}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{ramaswamy2019federated,
  title={Federated learning for emoji prediction in a mobile keyboard},
  author={Ramaswamy, Swaroop and Mathews, Rajiv and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
  journal={arXiv preprint arXiv:1906.04329},
  year={2019}
}

@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}

@article{carmon2020lower,
  title={Lower bounds for finding stationary points I},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={71--120},
  year={2020},
  publisher={Springer}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{zhao2021faster,
  title={Faster Rates for Compressed Federated Learning with Client-Variance Reduction},
  author={Zhao, Haoyu and Burlachenko, Konstantin and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2112.13097},
  year={2021}
}

@article{zhao2021fedpage,
  title={FedPAGE: A fast local stochastic gradient method for communication-efficient federated learning},
  author={Zhao, Haoyu and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2108.04755},
  year={2021}
}

@article{li2021zerosarah,
  title={ZeroSARAH: Efficient nonconvex finite-sum optimization with zero full gradient computation},
  author={Li, Zhize and Hanzely, Slavom{\'\i}r and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2103.01447},
  year={2021}
}